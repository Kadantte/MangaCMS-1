"""Migrate Hentai Data

Revision ID: 0ab876947815
Revises: fd657f7977c2
Create Date: 2018-02-23 22:30:17.727678

"""

# revision identifiers, used by Alembic.
revision = '0ab876947815'
down_revision = '1d6c79218b3e'
branch_labels = None
depends_on = None

import traceback
import sys
import threading
import concurrent.futures

from alembic import op
import sqlalchemy as sa

import sqlalchemy_utils
import sqlalchemy_jsonfield

# Patch in knowledge of the citext type, so it reflects properly.
from sqlalchemy.dialects.postgresql.base import ischema_names
import citext
import queue
import datetime
from sqlalchemy.dialects.postgresql import ENUM
from sqlalchemy.dialects.postgresql import JSON
from sqlalchemy.dialects.postgresql import TSVECTOR
ischema_names['citext'] = citext.CIText


import datetime
import cachetools
import tqdm
import json
import hashlib
import os.path

from alembic import op
import sqlalchemy as sa
import sqlalchemy_utils

# Patch in knowledge of the citext type, so it reflects properly.
from sqlalchemy.orm.session import Session
from sqlalchemy.dialects.postgresql.base import ischema_names
import citext
from sqlalchemy.dialects.postgresql import ENUM
from sqlalchemy.dialects.postgresql import JSON
from sqlalchemy.dialects.postgresql import TSVECTOR
ischema_names['citext'] = citext.CIText

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.orm import scoped_session
from sqlalchemy.orm import backref
from sqlalchemy.orm import joinedload
from sqlalchemy import Table
from sqlalchemy import Index

from sqlalchemy import Column
from sqlalchemy import Integer
from sqlalchemy import BigInteger
from sqlalchemy import Text
from sqlalchemy import Float
from sqlalchemy import Boolean
from sqlalchemy import DateTime
from sqlalchemy import ForeignKey
from sqlalchemy import PrimaryKeyConstraint
from sqlalchemy.orm import relationship
from sqlalchemy.schema import UniqueConstraint
import sqlalchemy_jsonfield

from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.ext.associationproxy import association_proxy

# Patch in knowledge of the citext type, so it reflects properly.
from sqlalchemy.dialects.postgresql.base import ischema_names
from sqlalchemy.dialects.postgresql import ENUM
from sqlalchemy.ext.declarative import declarative_base

dlstate_enum   = ENUM('new', 'fetching', 'processing', 'complete', 'error', 'removed', 'disabled', 'upload', 'missing', name='dlstate_enum')
dir_type       = ENUM('had_dir', 'created_dir', 'unknown', name='dirtype_enum')
file_type      = ENUM('manga', 'hentai', 'unknown', name='filetype_enum')


def upgrade():
	# ### commands auto generated by Alembic - please #

	import MangaCMSOld.lib.dbPool

	if "testing" in sys.argv:
		return

	def go(mode):
		bind = op.get_bind()
		sess = Session(bind=bind)

		# Cache 500K items, with a 30 minute ttl
		tag_cache_size = 500 * 1000
		tag_cache_ttl = 60 * 30

		########################################################################################

		Base = declarative_base()

		########################################################################################

		manga_files_tags_link = Table(
				'manga_files_tags_link', Base.metadata,
				Column('releases_id', Integer, ForeignKey('release_files.id'), nullable=False),
				Column('tags_id',     Integer, ForeignKey('manga_tags.id'),  nullable=False),
				PrimaryKeyConstraint('releases_id', 'tags_id')
			)
		manga_releases_tags_link = Table(
				'manga_releases_tags_link', Base.metadata,
				Column('releases_id', Integer, ForeignKey('manga_releases.id'), nullable=False),
				Column('tags_id',     Integer, ForeignKey('manga_tags.id'),  nullable=False),
				PrimaryKeyConstraint('releases_id', 'tags_id')
			)

		class MangaTags(Base):
			__tablename__ = 'manga_tags'
			id          = Column(Integer, primary_key=True)
			tag         = Column(citext.CIText(), nullable=False, index=True)

			__table_args__ = (
					UniqueConstraint('tag'),
				)

			@classmethod
			@cachetools.cached(cache=cachetools.TTLCache(tag_cache_size, tag_cache_ttl))
			def get_or_create(cls, tag):
				tmp = sess.query(cls)    \
					.filter(cls.tag == tag) \
					.scalar()
				if tmp:
					sess.expunge(tmp)
					return tmp

				# print("manga_tag_creator", tag)
				tmp = cls(tag=tag)
				sess.add(tmp)
				sess.commit()
				sess.expunge(tmp)
				return tmp


		########################################################################################

		class MangaReleases(Base):
			__tablename__ = 'manga_releases'
			id                  = Column(BigInteger, primary_key=True)
			state               = Column(dlstate_enum, nullable=False, index=True, default='new')
			err_str             = Column(Text)

			source_site         = Column(Text, nullable=False, index=True)  # Actual source site
			source_id           = Column(Text, nullable=False, index=True)  # ID On source site. Usually (but not always) the item URL

			first_seen          = Column(DateTime, nullable=False)
			posted_at           = Column(DateTime, nullable=False, default=datetime.datetime.min)
			downloaded_at       = Column(DateTime, nullable=False, default=datetime.datetime.min)
			last_checked        = Column(DateTime, nullable=False, default=datetime.datetime.min)

			deleted             = Column(Boolean, default=False, nullable=False)
			was_duplicate       = Column(Boolean, default=False, nullable=False)
			phash_duplicate     = Column(Boolean, default=False, nullable=False)
			uploaded            = Column(Boolean, default=False, nullable=False)

			dirstate            = Column(dir_type, nullable=False, default="unknown")

			origin_name         = Column(Text)
			series_name         = Column(Text, index=True)

			additional_metadata = Column(sqlalchemy_jsonfield.JSONField())

			fileid              = Column(BigInteger, ForeignKey('release_files.id'))
			file                = relationship('ReleaseFile', backref='manga_releases')

			tags_rel       = relationship('MangaTags',
												secondary        = manga_releases_tags_link,
												backref          = backref("manga_releases", lazy='dynamic'),
												collection_class = set)
			tags           = association_proxy('tags_rel', 'tag', creator=MangaTags.get_or_create)

			__table_args__ = (
					UniqueConstraint('source_site', 'source_id'),
					Index('manga_releases_source_site_id_idx', 'source_site', 'source_id')
				)




		class ReleaseFile(Base):
			__tablename__ = 'release_files'
			id             = Column(BigInteger, primary_key=True)

			dirpath        = Column(Text, nullable=False)
			filename       = Column(Text, nullable=False)
			fhash          = Column(Text, nullable=False)
			file_type      = Column(file_type, nullable=False, default="unknown")

			was_duplicate       = Column(Boolean, default=False, nullable=False)

			last_dup_check = Column(DateTime, nullable=False, default=datetime.datetime.min)

			manga_tags_rel       = relationship('MangaTags',
												secondary=manga_files_tags_link,
												backref=backref("release_files", lazy='dynamic'),
												collection_class=set)
			manga_tags           = association_proxy('manga_tags_rel', 'tag', creator=MangaTags.get_or_create)

			# releases       = relationship('MangaReleases')

			__table_args__ = (
				UniqueConstraint('dirpath', 'filename'),
				UniqueConstraint('fhash'),
				)


		def get_add_file(sess, fname, fpath):
			if fpath is None or fname is None:
				return None
			fqname = os.path.join(fpath, fname)
			if not os.path.exists(fqname):
				return None
			if os.path.isdir(fqname):
				return None

			have = sess.query(ReleaseFile)             \
				.filter(ReleaseFile.dirpath == fpath)  \
				.filter(ReleaseFile.filename == fname) \
				.scalar()

			if have:
				return have


			# print("Hashing file...", end="", flush=True)
			hash_md5 = hashlib.md5()
			with open(fqname, "rb") as f:
				hash_md5.update(f.read())
			fhash = hash_md5.hexdigest()
			# print("done.")


			have = sess.query(ReleaseFile)          \
				.filter(ReleaseFile.fhash == fhash) \
				.scalar()

			if have:
				# print("Have by fhash")
				return have

			new = ReleaseFile(
				dirpath  = fpath,
				filename = fname,
				fhash    = fhash
				)
			sess.add(new)
			return new


		def migrate_manga_tags(row, flags, tags):
			# print("Tags:", tags)
			tags = tags.split(" ")
			tags = [tmp for tmp in tags if not tmp.startswith("crosslink-")]

			tags = [tmp.replace("-(female)", "-female").replace("-(male)", "-male") for tmp in tags]
			tags = [tmp for tmp in tags if len(tmp) >= 2]

			tags = set(tags)
			tags = tags - set(["phash-duplicate", "was-duplicate", "uploaded", 'dup-checked', 'deleted'])

			if not tags:
				return

			if row.file:
				row.file.manga_tags.update(tags)
			row.tags.update(tags)

		def dlstate_decode(state_int):

			state_val = "new"
			if state_int == 1:
				state_val = 'fetching'
			elif state_int == 2:
				state_val = 'complete'
			elif state_int == 3:
				state_val = 'upload'
			elif state_int > 3:
				state_val = 'disabled'
			elif state_int < 0:
				state_val = 'error'
			return state_val

		def dirstate_decode(flags):

			dirstate_val = "unknown"
			if "haddir" in flags:
				dirstate_val = "had_dir"
			elif "new_dir" in flags:
				dirstate_val = "new_dir"
			return dirstate_val

		def go_manga():

			old_con = MangaCMSOld.lib.dbPool.pool.getconn()
			old_cur = old_con.cursor()

			# print("Connection:", old_con)
			# print("Cursor:", old_cur)
			# print("Session:", sess)
			old_cur.execute("SELECT sourcesite, dlstate, sourceurl, retreivaltime, lastupdate, sourceid, seriesname, filename, originname, downloadpath, flags, tags, note FROM mangaitems ORDER BY dbid DESC")

			fetchchunk = 1000
			items = []

			print("Loading %s rows from DB" % mode)
			items = old_cur.fetchall()
			print("Loaded %s %s rows" % (len(items), mode))

			print("Barrier released!")
			new = 0
			for item in tqdm.tqdm(items, desc="Processing %s" % mode, position=1 if mode == "hentai" else 0):
				sourcesite, dlstate, sourceurl, retreivaltime, lastupdate, sourceid, seriesname, filename, originname, downloadpath, flags, tags, note = item

				tags = tags if tags else ""
				flags = flags if flags else ""

				have = sess.query(MangaReleases)             \
					.filter(MangaReleases.source_id == sourceurl) \
					.options(joinedload("file"), joinedload("file.manga_tags_rel"), joinedload("tags_rel"), )  \
					.scalar()

				if have:
					migrate_manga_tags(have, flags, tags)
					# print("Skipping!")
					continue

				file = get_add_file(sess, filename, downloadpath)
				sess.flush()

				# print("'{}', '{}'".format(flags, tags))
				tags = tags if tags else ""
				flags = flags if flags else ""

				state_val = dlstate_decode(dlstate)
				dirstate_val = dirstate_decode(flags)


				additional_metadata = {}
				if file is None:
					state_val = 'missing'
					additional_metadata = {
						'filename'     : filename,
						'downloadpath' : downloadpath,
					}



				if sourceid:
					loaded_meta = json.loads(sourceid)

					additional_metadata['sourceid'] = loaded_meta
				if note:
					additional_metadata['note'] = note
					# print("Note:", note)

				dl_at     = datetime.datetime.utcfromtimestamp(lastupdate)
				posted_at = datetime.datetime.utcfromtimestamp(retreivaltime)

				row = MangaReleases(
						state               = state_val,
						err_str             = None,
						source_site         = sourcesite,
						series_name         = seriesname,
						source_id           = sourceurl,
						first_seen          = dl_at if dl_at > posted_at else posted_at,
						posted_at           = dl_at,
						downloaded_at       = posted_at,
						phash_duplicate     = "phash-duplicate" in tags,
						was_duplicate       = "was-duplicate" in tags,
						uploaded            = "uploaded" in tags,
						deleted             = "deleted" in tags,
						dirstate            = dirstate_val,
						origin_name         = originname,
						additional_metadata = additional_metadata,
						fileid              = file.id if file else None,
					)
				sess.add(row)
				migrate_manga_tags(row, flags, tags)

				new += 1
				if new > 1000:
					new = 0
					print("\nCommitting %s!\n" % mode)

					sess.flush()
					sess.commit()
					bind.execute("""COMMIT""")

		try:
			go_manga()
			print("Migration of %s complete!" % mode)
			return True
		except Exception as e:
			print("Thread had exception")
			traceback.print_exc()
			return False
			raise e

	with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
		res = []
		res.append(executor.submit(go, 'manga'))

		if not all([tmp.result() for tmp in res]):

			raise RuntimeError("Failure in migration!")

	# ### end Alembic commands ###

def downgrade():
	# ### commands auto generated by Alembic - please adjust! ###
	pass
	# ### end Alembic commands ###
